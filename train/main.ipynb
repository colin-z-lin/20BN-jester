{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model with sampel data #\n",
    "This code is based on the code here: https://github.com/anasmorahhib/3D-CNN-Gesture-recognition/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Imports ####################\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/clin/Springboard/Capstone/Jester/Samples'\n",
    "TRAIN_DATA = BASE_DIR + '/train.csv'\n",
    "TRAIN_VIDS = BASE_DIR + '/train'\n",
    "VALID_DATA = BASE_DIR + '/validation.csv'\n",
    "VALID_VIDS = BASE_DIR + '/validation'\n",
    "FRAME_COUNT = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the RAM\n",
    "def release_list(a):\n",
    "   del a[:]\n",
    "   del a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata file, which is a csv with 2 columns.\n",
    "# The first column being the id of the video, and the second\n",
    "# column being the label of the video\n",
    "\n",
    "def read_metadata(file_path):\n",
    "    data = pd.read_csv(file_path, header = None, sep = \";\")\n",
    "    data = data.set_index(0)[1].to_dict()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all vidoes have the same number of frames\n",
    "def get_unify_frames(video_path, hm_frames = FRAME_COUNT):\n",
    "    offset = 0\n",
    "    # pick frames\n",
    "    frames = os.listdir(video_path)\n",
    "    frames_count = len(frames)\n",
    "    # unify number of frames \n",
    "    if hm_frames > frames_count:\n",
    "        # duplicate last frame if video is shorter than necessary\n",
    "        frames += [frames[-1]] * (hm_frames - frames_count)\n",
    "    elif hm_frames < frames_count:\n",
    "        # If there are more frames, then sample starting offset\n",
    "        #diff = (frames_count - hm_frames)\n",
    "        #offset = diff-1 \n",
    "        frames = frames[0:hm_frames]\n",
    "    return frames  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize frames\n",
    "def resize_frame(frame):\n",
    "    frame = img.imread(frame)\n",
    "    frame = cv2.resize(frame, (64, 64))\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "def normalize(data):\n",
    "    print('old mean', data.mean())\n",
    "    scaler = StandardScaler()\n",
    "    scaled_images  = scaler.fit_transform(data.reshape(-1, FRAME_COUNT * 64 * 64))\n",
    "    print('new mean', scaled_images.mean())\n",
    "    scaled_images  = scaled_images.reshape(-1, FRAME_COUNT, 64, 64, 1)\n",
    "    print(scaled_images.shape)\n",
    "    return scaled_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(parent_dir_for_videos, labels, metadata, show_img = False):\n",
    "    dirs = os.listdir(parent_dir_for_videos)\n",
    "\n",
    "    # Adjust training data\n",
    "    training_targets = [] # training targets \n",
    "    new_frames = [] # training data after resize & unify\n",
    "    for directory in dirs:\n",
    "        new_frame = [] # one training\n",
    "        # Frames in each folder\n",
    "        frames = get_unify_frames(os.path.join(parent_dir_for_videos, directory))\n",
    "        for frame in frames:\n",
    "            frame = resize_frame(os.path.join(parent_dir_for_videos, directory, frame))\n",
    "            new_frame.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "        new_frames.append(new_frame)\n",
    "        training_targets.append(labels.index(metadata[int(directory)]))\n",
    "\n",
    "    if show_img:\n",
    "        #show data\n",
    "        fig = plt.figure()\n",
    "        for i in range(2, 4):\n",
    "            for num, frame in enumerate(new_frames[i][0:18]):\n",
    "                y = fig.add_subplot(4, 5, num + 1)\n",
    "                y.imshow(frame, cmap = 'gray')\n",
    "            fig = plt.figure()\n",
    "        plt.show()\n",
    "\n",
    "    new_frames_in_narray = np.array(new_frames[:], dtype = np.float32)\n",
    "    release_list(new_frames)\n",
    "    scaled_images = normalize(new_frames_in_narray)\n",
    "\n",
    "    return scaled_images, training_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old mean 108.18764\n",
      "new mean 4.504909e-09\n",
      "(8246, 30, 64, 64, 1)\n",
      "old mean 105.743706\n",
      "new mean 4.006081e-09\n",
      "(980, 30, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "train_metadata = read_metadata(TRAIN_DATA)\n",
    "valid_metadata = read_metadata(VALID_DATA)\n",
    "labels = list(set(train_metadata.values()))\n",
    "\n",
    "# Get the data directories\n",
    "\n",
    "train_images, train_labels = load_video_frames(TRAIN_VIDS, labels, train_metadata)\n",
    "valid_images, valid_labels = load_video_frames(VALID_VIDS, labels, valid_metadata)\n",
    "\n",
    "x_train = np.array(train_images)\n",
    "y_train = np.array(train_labels)\n",
    "x_val = np.array(valid_images)\n",
    "y_val = np.array(valid_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.7036 - accuracy: 0.5314 \n",
      "Epoch 00001: saving model to training_20200718/cp-0001.ckpt\n",
      "258/258 [==============================] - 3581s 14s/step - loss: 0.7036 - accuracy: 0.5314 - val_loss: 0.7144 - val_accuracy: 0.6031\n",
      "Epoch 2/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.8274 \n",
      "Epoch 00002: saving model to training_20200718/cp-0002.ckpt\n",
      "258/258 [==============================] - 3568s 14s/step - loss: 0.3944 - accuracy: 0.8274 - val_loss: 0.3943 - val_accuracy: 0.8276\n",
      "Epoch 3/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9010 \n",
      "Epoch 00003: saving model to training_20200718/cp-0003.ckpt\n",
      "258/258 [==============================] - 3570s 14s/step - loss: 0.2431 - accuracy: 0.9010 - val_loss: 0.2542 - val_accuracy: 0.9163\n",
      "Epoch 4/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9361 \n",
      "Epoch 00004: saving model to training_20200718/cp-0004.ckpt\n",
      "258/258 [==============================] - 3571s 14s/step - loss: 0.1613 - accuracy: 0.9361 - val_loss: 0.2160 - val_accuracy: 0.9306\n",
      "Epoch 5/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9554 \n",
      "Epoch 00005: saving model to training_20200718/cp-0005.ckpt\n",
      "258/258 [==============================] - 3579s 14s/step - loss: 0.1156 - accuracy: 0.9554 - val_loss: 0.2249 - val_accuracy: 0.9306\n",
      "Epoch 6/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9675 \n",
      "Epoch 00006: saving model to training_20200718/cp-0006.ckpt\n",
      "258/258 [==============================] - 3584s 14s/step - loss: 0.0882 - accuracy: 0.9675 - val_loss: 0.2198 - val_accuracy: 0.9378\n",
      "Epoch 7/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9766 \n",
      "Epoch 00007: saving model to training_20200718/cp-0007.ckpt\n",
      "258/258 [==============================] - 3577s 14s/step - loss: 0.0611 - accuracy: 0.9766 - val_loss: 0.2484 - val_accuracy: 0.9388\n",
      "Epoch 8/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9842 \n",
      "Epoch 00008: saving model to training_20200718/cp-0008.ckpt\n",
      "258/258 [==============================] - 3575s 14s/step - loss: 0.0441 - accuracy: 0.9842 - val_loss: 0.2821 - val_accuracy: 0.9316\n",
      "Epoch 9/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9845 \n",
      "Epoch 00009: saving model to training_20200718/cp-0009.ckpt\n",
      "258/258 [==============================] - 3576s 14s/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.2638 - val_accuracy: 0.9449\n",
      "Epoch 10/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9908 \n",
      "Epoch 00010: saving model to training_20200718/cp-0010.ckpt\n",
      "258/258 [==============================] - 3568s 14s/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.3044 - val_accuracy: 0.9378\n",
      "Epoch 11/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9925 \n",
      "Epoch 00011: saving model to training_20200718/cp-0011.ckpt\n",
      "258/258 [==============================] - 3563s 14s/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.3092 - val_accuracy: 0.9429\n",
      "Epoch 12/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9890 \n",
      "Epoch 00012: saving model to training_20200718/cp-0012.ckpt\n",
      "258/258 [==============================] - 3554s 14s/step - loss: 0.0312 - accuracy: 0.9890 - val_loss: 0.3197 - val_accuracy: 0.9418\n",
      "Epoch 13/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9919 \n",
      "Epoch 00013: saving model to training_20200718/cp-0013.ckpt\n",
      "258/258 [==============================] - 3562s 14s/step - loss: 0.0237 - accuracy: 0.9919 - val_loss: 0.3712 - val_accuracy: 0.9378\n",
      "Epoch 14/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9960 \n",
      "Epoch 00014: saving model to training_20200718/cp-0014.ckpt\n",
      "258/258 [==============================] - 3573s 14s/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 0.3479 - val_accuracy: 0.9429\n",
      "Epoch 15/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9964 \n",
      "Epoch 00015: saving model to training_20200718/cp-0015.ckpt\n",
      "258/258 [==============================] - 3575s 14s/step - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.4399 - val_accuracy: 0.9235\n",
      "Epoch 16/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9931 \n",
      "Epoch 00016: saving model to training_20200718/cp-0016.ckpt\n",
      "258/258 [==============================] - 3571s 14s/step - loss: 0.0199 - accuracy: 0.9931 - val_loss: 0.4020 - val_accuracy: 0.9378\n",
      "Epoch 17/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9901 \n",
      "Epoch 00017: saving model to training_20200718/cp-0017.ckpt\n",
      "258/258 [==============================] - 3582s 14s/step - loss: 0.0267 - accuracy: 0.9901 - val_loss: 0.3506 - val_accuracy: 0.9276\n",
      "Epoch 18/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9909 \n",
      "Epoch 00018: saving model to training_20200718/cp-0018.ckpt\n",
      "258/258 [==============================] - 3574s 14s/step - loss: 0.0262 - accuracy: 0.9909 - val_loss: 0.3768 - val_accuracy: 0.9357\n",
      "Epoch 19/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9951 \n",
      "Epoch 00019: saving model to training_20200718/cp-0019.ckpt\n",
      "258/258 [==============================] - 3574s 14s/step - loss: 0.0143 - accuracy: 0.9951 - val_loss: 0.4283 - val_accuracy: 0.9408\n",
      "Epoch 20/20\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9967 \n",
      "Epoch 00020: saving model to training_20200718/cp-0020.ckpt\n",
      "258/258 [==============================] - 3586s 14s/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.3723 - val_accuracy: 0.9378\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "todays_date = datetime.now().strftime('%Y%m%d')\n",
    "model = Conv3DModel()\n",
    "\n",
    "# choose the loss and optimizer methods\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# include the epoch in the file name. (uses `str.format`)\n",
    "checkpoint_path = \"training_\" + todays_date + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "\n",
    "# Run the training \n",
    "history = model.fit(x_train, y_train,\n",
    "                    callbacks = [cp_callback],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    batch_size=32,\n",
    "                    epochs=20)\n",
    "\n",
    "\n",
    "# save the model for use in the application\n",
    "model.save_weights('weights_{}/my_weights'.format(todays_date), save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
