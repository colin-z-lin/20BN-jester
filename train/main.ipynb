{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model with sampel data #\n",
    "This code is based on the code here: https://github.com/anasmorahhib/3D-CNN-Gesture-recognition/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Imports ####################\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the RAM\n",
    "def release_list(a):\n",
    "   del a[:]\n",
    "   del a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata file, which is a csv with 2 columns.\n",
    "# The first column being the id of the video, and the second\n",
    "# column being the label of the video\n",
    "\n",
    "def read_metadata(file_path):\n",
    "    data = pd.read_csv(file_path, header = None, sep = \";\")\n",
    "    data = data.set_index(0)[1].to_dict()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all vidoes have the same number of frames\n",
    "def get_unify_frames(video_path, hm_frames = 30):\n",
    "    offset = 0\n",
    "    # pick frames\n",
    "    frames = os.listdir(video_path)\n",
    "    frames_count = len(frames)\n",
    "    # unify number of frames \n",
    "    if hm_frames > frames_count:\n",
    "        # duplicate last frame if video is shorter than necessary\n",
    "        frames += [frames[-1]] * (hm_frames - frames_count)\n",
    "    elif hm_frames < frames_count:\n",
    "        # If there are more frames, then sample starting offset\n",
    "        #diff = (frames_count - hm_frames)\n",
    "        #offset = diff-1 \n",
    "        frames = frames[0:hm_frames]\n",
    "    return frames  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize frames\n",
    "def resize_frame(frame):\n",
    "    frame = img.imread(frame)\n",
    "    frame = cv2.resize(frame, (64, 64))\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "def normalize(data):\n",
    "    print('old mean', data.mean())\n",
    "    scaler = StandardScaler()\n",
    "    scaled_images  = scaler.fit_transform(data.reshape(-1, 15*64*64))\n",
    "    print('new mean', scaled_images.mean())\n",
    "    scaled_images  = scaled_images.reshape(-1, 15, 64, 64, 1)\n",
    "    print(scaled_images.shape)\n",
    "    return scaled_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(parent_dir_for_videos, labels, metadata, show_img = False):\n",
    "    dirs = os.listdir(parent_dir_for_videos)\n",
    "\n",
    "    # Adjust training data\n",
    "    training_targets = [] # training targets \n",
    "    new_frames = [] # training data after resize & unify\n",
    "    for directory in dirs:\n",
    "        new_frame = [] # one training\n",
    "        # Frames in each folder\n",
    "        frames = get_unify_frames(os.path.join(parent_dir_for_videos, directory))\n",
    "        for frame in frames:\n",
    "            frame = resize_frame(os.path.join(parent_dir_for_videos, directory, frame))\n",
    "            new_frame.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "            if len(new_frame) == 15: # partition each training on two trainings.\n",
    "                new_frames.append(new_frame) # append each partition to training data\n",
    "                training_targets.append(labels.index(metadata[int(directory)]))\n",
    "                new_frame = []\n",
    "\n",
    "    if show_img:\n",
    "        #show data\n",
    "        fig = plt.figure()\n",
    "        for i in range(2, 4):\n",
    "            for num, frame in enumerate(new_frames[i][0:18]):\n",
    "                y = fig.add_subplot(4, 5, num + 1)\n",
    "                y.imshow(frame, cmap = 'gray')\n",
    "            fig = plt.figure()\n",
    "        plt.show()\n",
    "\n",
    "    new_frames_in_narray = np.array(new_frames[:], dtype = np.float32)\n",
    "    release_list(new_frames)\n",
    "    scaled_images = normalize(new_frames_in_narray)\n",
    "\n",
    "    return scaled_images, training_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/clin/Springboard/Capstone/Jester/Samples'\n",
    "TRAIN_DATA = BASE_DIR + '/train.csv'\n",
    "TRAIN_VIDS = BASE_DIR + '/train'\n",
    "VALID_DATA = BASE_DIR + '/validation.csv'\n",
    "VALID_VIDS = BASE_DIR + '/validation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old mean 108.18764\n",
      "new mean -6.7296924e-09\n",
      "(16492, 15, 64, 64, 1)\n",
      "old mean 105.743706\n",
      "new mean -8.016216e-09\n",
      "(1960, 15, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "train_metadata = read_metadata(TRAIN_DATA)\n",
    "valid_metadata = read_metadata(VALID_DATA)\n",
    "labels = list(set(train_metadata.values()))\n",
    "\n",
    "# Get the data directories\n",
    "\n",
    "train_images, train_labels = load_video_frames(TRAIN_VIDS, labels, train_metadata)\n",
    "valid_images, valid_labels = load_video_frames(VALID_VIDS, labels, valid_metadata)\n",
    "\n",
    "x_train = np.array(train_images)\n",
    "y_train = np.array(train_labels)\n",
    "x_val = np.array(valid_images)\n",
    "y_val = np.array(valid_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.5564 - accuracy: 0.6882\n",
      "Epoch 00001: saving model to training_20200714/cp-0001.ckpt\n",
      "516/516 [==============================] - 2870s 6s/step - loss: 0.5564 - accuracy: 0.6882 - val_loss: 0.4346 - val_accuracy: 0.8051\n",
      "Epoch 2/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.8628\n",
      "Epoch 00002: saving model to training_20200714/cp-0002.ckpt\n",
      "516/516 [==============================] - 2842s 6s/step - loss: 0.3261 - accuracy: 0.8628 - val_loss: 0.3674 - val_accuracy: 0.8495\n",
      "Epoch 3/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9075\n",
      "Epoch 00003: saving model to training_20200714/cp-0003.ckpt\n",
      "516/516 [==============================] - 2841s 6s/step - loss: 0.2218 - accuracy: 0.9075 - val_loss: 0.2650 - val_accuracy: 0.8959\n",
      "Epoch 4/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9348\n",
      "Epoch 00004: saving model to training_20200714/cp-0004.ckpt\n",
      "516/516 [==============================] - 2831s 5s/step - loss: 0.1633 - accuracy: 0.9348 - val_loss: 0.2656 - val_accuracy: 0.9138\n",
      "Epoch 5/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9536\n",
      "Epoch 00005: saving model to training_20200714/cp-0005.ckpt\n",
      "516/516 [==============================] - 2826s 5s/step - loss: 0.1222 - accuracy: 0.9536 - val_loss: 0.2699 - val_accuracy: 0.9082\n",
      "Epoch 6/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9640\n",
      "Epoch 00006: saving model to training_20200714/cp-0006.ckpt\n",
      "516/516 [==============================] - 2844s 6s/step - loss: 0.0926 - accuracy: 0.9640 - val_loss: 0.3093 - val_accuracy: 0.9056\n",
      "Epoch 7/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9752\n",
      "Epoch 00007: saving model to training_20200714/cp-0007.ckpt\n",
      "516/516 [==============================] - 2837s 5s/step - loss: 0.0665 - accuracy: 0.9752 - val_loss: 0.3404 - val_accuracy: 0.9061\n",
      "Epoch 8/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9824\n",
      "Epoch 00008: saving model to training_20200714/cp-0008.ckpt\n",
      "516/516 [==============================] - 2828s 5s/step - loss: 0.0473 - accuracy: 0.9824 - val_loss: 0.3560 - val_accuracy: 0.8974\n",
      "Epoch 9/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9837\n",
      "Epoch 00009: saving model to training_20200714/cp-0009.ckpt\n",
      "516/516 [==============================] - 2825s 5s/step - loss: 0.0456 - accuracy: 0.9837 - val_loss: 0.3953 - val_accuracy: 0.8964\n",
      "Epoch 10/10\n",
      "516/516 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9888\n",
      "Epoch 00010: saving model to training_20200714/cp-0010.ckpt\n",
      "516/516 [==============================] - 2824s 5s/step - loss: 0.0312 - accuracy: 0.9888 - val_loss: 0.4460 - val_accuracy: 0.9026\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "todays_date = datetime.now().strftime('%Y%m%d')\n",
    "model = Conv3DModel()\n",
    "\n",
    "# choose the loss and optimizer methods\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# include the epoch in the file name. (uses `str.format`)\n",
    "checkpoint_path = \"training_\" + todays_date + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "\n",
    "# Run the training \n",
    "history = model.fit(x_train, y_train,\n",
    "                    callbacks = [cp_callback],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    batch_size=32,\n",
    "                    epochs=10)\n",
    "\n",
    "\n",
    "# save the model for use in the application\n",
    "model.save_weights('weights_{}/my_weights'.format(todays_date), save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
